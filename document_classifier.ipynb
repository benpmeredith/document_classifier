{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import glob\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data\n",
    "text_df = pd.DataFrame()\n",
    "labels = pd.DataFrame()\n",
    "fp_base = './Data_Categorical/'\n",
    "violence_types = ['Physical violence', 'Bullying', 'Weapons use', 'Dating violence']\n",
    "data = []\n",
    "for vtype in violence_types:\n",
    "    fp_complete = fp_base + vtype + '/*.txt'\n",
    "    files_list = glob.glob(fp_complete)\n",
    "    for fname in files_list:\n",
    "        with open(fname, errors='ignore') as f:\n",
    "            txt = f.read()\n",
    "        data.append((txt, vtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    \"\"\"lemmatize and clean doc\"\"\"\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return ''.join([x for x in normalized if not x.isnumeric()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "label = []\n",
    "# *NOTE: Better to use zip(*data)\n",
    "for (t, l) in data:\n",
    "    text.append(clean(t))\n",
    "    label.append(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try TF->SVD->SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = ['dating', 'bullying', 'violence', 'sexual', 'weapons']\n",
    "x_train, x_test, y_train, y_test = train_test_split(text, label, test_size=0.2)\n",
    "tf_vect = TfidfVectorizer(use_idf=False, binary=True, max_features=100, ngram_range=(2,2))\n",
    "x_train_tf = tf_vect.fit_transform(x_train)\n",
    "x_test_tf = tf_vect.transform(x_test)\n",
    "svd = TruncatedSVD(n_components=3)\n",
    "x_train_tf_svd = svd.fit_transform(x_train_tf)\n",
    "x_test_tf_svd = svd.transform(x_test_tf)\n",
    "clf = LinearSVC()\n",
    "clf.fit(x_train_tf, y_train)\n",
    "print(\"******Test Result******\")\n",
    "print(\"Test Set Size:\", len(x_test))\n",
    "print(\"Score:\", sum(clf.predict(x_test_tf) == y_test) / len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to use matrix distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dating_violence_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "dating_violence_texts = [x[0] for x in data if x[1] == 'Dating violence']\n",
    "dating_vectorizer = TfidfVectorizer(binary=True, max_features=100)\n",
    "model_vector = dating_vectorizer.fit_transform(dating_violence_texts[:20])\n",
    "test_vector = dating_vectorizer.transform(dating_violence_texts[21:])\n",
    "print(type(dating_violence_texts[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_average = np.mean(model_vector, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(distance.cdist(vector_average, test_vector[6].toarray(), 'euclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weapon_text = [x[0] for x in data if x[1] == 'Weapons use']\n",
    "test_weapon_vector = dating_vectorizer.transform(weapon_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weapon_means = []\n",
    "for tv in test_weapon_vector:\n",
    "    weapon_means.append(np.mean(distance.cdist(vector_average, tv.toarray(), 'euclidean')))\n",
    "weapon_means = np.array(weapon_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(weapon_means)\n",
    "plt.xlim(0,1)\n",
    "print(len(weapon_means[weapon_means <= 0.25]) / len(weapon_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dating_means = []\n",
    "for tv in test_vector: # dating\n",
    "    dating_means.append(np.mean(distance.cdist(vector_average, tv.toarray(), 'euclidean')))\n",
    "dating_means = np.array(dating_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dating_means)\n",
    "plt.xlim(0,1)\n",
    "print(len(dating_means[dating_means <= 0.25])/ len(dating_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_text = [x[0] for x in data if x[1] == 'Physical violence']\n",
    "test_phys_vector = dating_vectorizer.transform(phys_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_means = []\n",
    "for tv in test_phys_vector:\n",
    "    phys_means.append(np.mean(distance.cdist(vector_average, tv.toarray(), 'euclidean')))\n",
    "phys_means = np.array(phys_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(phys_means)\n",
    "plt.xlim(0,1)\n",
    "print(len(phys_means[phys_means <= 0.25])/ len(phys_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_text = [x[0] for x in data if x[1] == 'Physical violence']\n",
    "dating_text = [x[0] for x in data if x[1] == 'Dating violence']\n",
    "bully_text = [x[0] for x in data if x[1] == 'Bullying']\n",
    "weapon_text = [x[0] for x in data if x[1] == 'Weapons use']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vector_distance_classifier import VDC\n",
    "dating_clf = VDC()\n",
    "dating_clf.fit(dating_text)\n",
    "phys_clf = VDC()\n",
    "phys_clf.fit(phys_text)\n",
    "weapons_clf = VDC()\n",
    "weapons_clf.fit(weapon_text)\n",
    "bully_clf = VDC()\n",
    "bully_clf.fit(bully_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from vector_distance_classifier import VDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(clf, data):\n",
    "    kf = KFold(n_splits=4, random_state=123)\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        #print(train_index)\n",
    "        #print(test_index)\n",
    "        clf.fit(np.array(data)[train_index].tolist())\n",
    "        predicted = clf.predict(np.array(data)[test_index].tolist())\n",
    "        predicted_np = np.array(predicted)\n",
    "        #print(predicted)\n",
    "        print(\"**Result:\", sum(predicted_np) / len(test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5287356321839081"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bully_clf.predict(weapon_text)) / len(weapon_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index.__type__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
